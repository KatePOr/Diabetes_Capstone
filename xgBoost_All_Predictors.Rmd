---
---
title: "xgBoost Model for All Variables"
output: pdf_document
---

```{r setup, include=FALSE}
library(caret)
library(readr)
library(ggplot2)
library(pROC)
library(tidymodels)
library(xgboost)
library(dplyr)
library(tidymodels) 
library(vip)
```

# Set seed
```{r}
set.seed(123)
```

# Preprocessing

## Read in dataset - KO
```{r}
CDC_2023_subset <- read_csv("CDC_2023_cleaned.csv")
```

## Remove unused columns - KO
```{r}
CDC_2023_subset <-  subset(CDC_2023_subset, select = -c(1, 5:6, 22, 24) )
```

## Convert columns (except for EDUCA, EMPLOY1, INCOME3, PHYSHLTH, MENTHLTH, DRNK3GE5, _AGE80, BMI) to factors - KO
```{r}
col_names <- colnames(CDC_2023_subset)
col_names <- col_names[-c(1:3, 5:6, 10, 19, 20)] # Column 1 (EDUCA), 2 (EMPLOY1), 3 (INCOME3), 5 (PHYSHLTH), 6 (MENTHLTH), 10 (DRNK3GE5), 19 (_AGE80), 20 (BMI)
CDC_2023_subset[,col_names] <- lapply(CDC_2023_subset[,col_names] , factor)
```

## Function to normalize BMI column - KO
```{r}
# Function to normalize specific columns based on dataset and columns specified
normalize <- function(dataset, columns) {
  scale(dataset[, columns],
  center = apply(dataset[, columns], 2, mean),
  scale = apply(dataset[, columns], 2, sd)
  )
}
```

## Split dataset into training and test sets - KO
Dataset will be split into 77% training and 23% testing.
```{r}
training <- createDataPartition(CDC_2023_subset$DIABETE4,
                                         p = 0.77,
                                         list = FALSE,
                                         times = 1)

CDC_2023_training <- CDC_2023_subset[training, ]
CDC_2023_test <- CDC_2023_subset[-training, ]
```

## Normalize Numeric Columns in training and test sets - KO
```{r}
columns = c(1:3, 5:6, 10, 19, 20)
CDC_2023_training[, columns] <- normalize(CDC_2023_training, columns)
CDC_2023_test[, columns] <- normalize(CDC_2023_test, columns)
```

# xgBoost 
```{r}
# using the standard predictive analytics/machine learning approach with the tidymodels framework 
data_split <- initial_split(CDC_2023_subset, strata = "DIABETE4", prop = 0.77)

CDC_2023_training <- training(data_split)
CDC_2023_test  <- testing(data_split)

# Normalize numeric and ordinal columns
columns = c(1:3, 5:6, 10, 19, 20)
CDC_2023_training[, columns] <- normalize(CDC_2023_training, columns)
CDC_2023_test[, columns] <- normalize(CDC_2023_test, columns)

# Calculate Weights for rf_pred
CDC_2023_training <-
  CDC_2023_training %>% 
  mutate(
    case_wts = ifelse(DIABETE4 == "1", 18, 1),
    case_wts = importance_weights(case_wts)
  )

cv_set <- vfold_cv(CDC_2023_training, strata = "DIABETE4", v = 5)

diabetes_recipe <- 
  recipe(
    DIABETE4 ~ ., 
    data = CDC_2023_training
  ) %>%
  step_dummy(all_nominal_predictors()) 

# Prep the recipe
diabetes_recipe_prepped <- prep(diabetes_recipe)

xgboost_model <- boost_tree(
      trees = tune(),
      tree_depth = tune(),
      learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(diabetes_recipe) %>%
  add_model(xgboost_model) %>%
  add_case_weights(case_wts)

################### use tune_grid to find best hyperparameter values #####################

xgboost_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  learn_rate(),
  size = 10
)

xgboost_results <- xgboost_workflow %>%
  tune_grid(resamples = cv_set,
            grid = xgboost_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

xgboost_results %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  dplyr::select(mean, trees:learn_rate) %>%
  pivot_longer(trees:learn_rate,
               values_to = "value",
               names_to = "parameter"
  )

xgboost_results %>% 
  show_best(metric = "roc_auc")

########################## use best hyperparameter values ################################

best_auc <- xgboost_results %>% select_best("roc_auc")
best_auc
```

# Best model
```{r}
xgboost_model_final <- boost_tree(
      trees = 197,
      tree_depth = 5,
      learn_rate = 0.02341828) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgboost_workflow <- 
  workflow() %>% 
  add_recipe(diabetes_recipe) %>%
  add_model(xgboost_model_final) %>%
  add_case_weights(case_wts)

last_xgboost_fit <- xgboost_workflow %>%
  fit(data = CDC_2023_training)

# Bake the test dataset
test_baked <- bake(diabetes_recipe_prepped, new_data = CDC_2023_test)

# Generate predictions on the baked test data
test_results <- 
  test_baked %>%
  bind_cols(
    predict(last_xgboost_fit, new_data = CDC_2023_test, type = "prob") %>%
      dplyr::rename(p_1 = .pred_1)  # Rename predicted probabilities
    ) %>%
  dplyr::mutate(DIABETE4 = CDC_2023_test$DIABETE4)  # Add true labels for evaluation

################################### feature importance ###################################

last_xgboost_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20)

##################################### plot ROC curve #####################################

roc_data <- data.frame(threshold=seq(1,0,-0.01), fpr=0, tpr=0)
for (i in roc_data$threshold) {
  
  over_threshold <- test_results[test_results$p_1 >= i, ]
  
  fpr <- sum(over_threshold$DIABETE4==0)/sum(test_results$DIABETE4==0)
  roc_data[roc_data$threshold==i, "fpr"] <- fpr
  
  tpr <- sum(over_threshold$DIABETE4==1)/sum(test_results$DIABETE4==1)
  roc_data[roc_data$threshold==i, "tpr"] <- tpr
  
}

ggplot() +
  geom_line(data = roc_data, aes(x = fpr, y = tpr, color = threshold), linewidth = 2) +
  scale_color_gradientn(colors = rainbow(3)) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = fpr, y = tpr)) +
  geom_text(data = roc_data[seq(1, 101, 10), ],
            aes(x = fpr, y = tpr, label = threshold, hjust = 1.2, vjust = -0.2)) +
  theme_bw() +
  xlab("FPR") + 
  ylab("TPR")


############################# ROC curve calculation breakdown ############################

threshold <- 0.5

test_results$predictions <- ifelse(test_results$p_1 >= threshold, 1, 0)
tp <- nrow(test_results[test_results$DIABETE4==1 & test_results$predictions==1, ])
paste("True Positive:", tp)
fp <- nrow(test_results[test_results$DIABETE4==0 & test_results$predictions==1, ])
paste("False Positive:", fp)
tn <- nrow(test_results[test_results$DIABETE4==0 & test_results$predictions==0, ])
paste("True Negative:", tn)
fn <- nrow(test_results[test_results$DIABETE4==1 & test_results$predictions==0, ])
paste("False Negative:", fn)

test_results$type <- ""
test_results[test_results$DIABETE4==1 & test_results$predictions==1, "type"] <- "tp"
test_results[test_results$DIABETE4==0 & test_results$predictions==1, "type"] <- "fp"
test_results[test_results$DIABETE4==0 & test_results$predictions==0, "type"] <- "tn"
test_results[test_results$DIABETE4==1 & test_results$predictions==0, "type"] <- "fn"

fpr <- fp/(fp + tn)
tpr <- tp/(tp + fn)

acc <- (tp + tn) / (tp + tn + fp + fn)
paste("Accuracy: ", acc)

###################################          AUC           ###################################

roc_object <- roc( CDC_2023_test$DIABETE4, test_results$p_1)
auc(roc_object)

################################### plot calibration curve ###################################

calibration_data <- data.frame(bin_midpoint=seq(0.05,0.95,0.1),
                               observed_event_percentage=0)
for (i in seq(0.05,0.95,0.1)) {
  
  in_interval <- test_results[test_results$p_1 >= (i-0.05) & test_results$p_1 <= (i+0.05), ]
  oep <- nrow(in_interval[in_interval$DIABETE4==1, ])/nrow(in_interval)
  calibration_data[calibration_data$bin_midpoint==i, "observed_event_percentage"] <- oep
  
}

ggplot(data = calibration_data, aes(x = bin_midpoint, y = observed_event_percentage)) +
  geom_line(linewidth = 1) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(size = 2) +
  geom_text(aes(label = bin_midpoint), hjust = 0.75, vjust = -0.5) +
  xlab("Bin Midpoint") +
  ylab("Observed Event Percentage") + 
  theme_bw() 
```

Old code:
final_xgboost_workflow <- finalize_workflow(
  xgboost_workflow,
  best_auc
)

last_xgboost_fit <- 
  final_xgboost_workflow %>% 
  last_fit(data_split)